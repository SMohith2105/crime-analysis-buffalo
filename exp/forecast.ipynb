{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rithv\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "19:00:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:00:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:00:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "19:01:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "19:01:10 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and forecasts saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('..\/db\/incidents.db')\n",
    "# Fetch data from the database\n",
    "query = \"SELECT * FROM incidents WHERE incident_datetime >= '2009-01-01'\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Convert incident_datetime to datetime objects\n",
    "df['incident_datetime'] = pd.to_datetime(df['incident_datetime'])\n",
    "\n",
    "# Assume df_filtered is the preprocessed dataframe from previous steps\n",
    "df_filtered = df  # You may need to add your preprocessing steps here\n",
    "\n",
    "# Aggregate data by date and neighborhood\n",
    "df_filtered['date'] = df_filtered['incident_datetime'].dt.date\n",
    "crime_data = df_filtered.groupby(['date', 'neighborhood']).size().reset_index(name='crime_count')\n",
    "\n",
    "# Prepare data for Prophet\n",
    "crime_data = crime_data.rename(columns={'date': 'ds', 'crime_count': 'y'})\n",
    "\n",
    "# Function to create and fit a Prophet model\n",
    "def fit_prophet_model(data):\n",
    "    model = Prophet()\n",
    "    model.fit(data)\n",
    "    return model\n",
    "\n",
    "# Function to create future dataframe and make predictions\n",
    "def make_forecast(model, periods=365):\n",
    "    future = model.make_future_dataframe(periods=periods)\n",
    "    forecast = model.predict(future)\n",
    "    return forecast\n",
    "\n",
    "# Create a dictionary to store models and forecasts for each neighborhood\n",
    "models = {}\n",
    "forecasts = {}\n",
    "\n",
    "neighborhoods = df_filtered['neighborhood'].unique()\n",
    "\n",
    "# Fit a model for each neighborhood\n",
    "for neighborhood in neighborhoods:\n",
    "    neighborhood_data = crime_data[crime_data['neighborhood'] == neighborhood]\n",
    "    if len(neighborhood_data) > 0:  # Ensure there is data for the neighborhood\n",
    "        model = fit_prophet_model(neighborhood_data)\n",
    "        forecast = make_forecast(model)\n",
    "        models[neighborhood] = model\n",
    "        forecasts[neighborhood] = forecast\n",
    "\n",
    "# Fit a model for the whole of Buffalo\n",
    "buffalo_data = df_filtered.groupby('date').size().reset_index(name='crime_count')\n",
    "buffalo_data = buffalo_data.rename(columns={'date': 'ds', 'crime_count': 'y'})\n",
    "\n",
    "buffalo_model = fit_prophet_model(buffalo_data)\n",
    "buffalo_forecast = make_forecast(buffalo_model)\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Create the 'data/forecast' folder if it doesn't exist\n",
    "forecast_directory = os.path.join(current_dir, '..', 'data', 'forecast')\n",
    "if not os.path.exists(forecast_directory):\n",
    "    os.makedirs(forecast_directory)\n",
    "\n",
    "# Save the Buffalo forecast\n",
    "buffalo_forecast_path = os.path.join(forecast_directory, 'buffalo_crime_forecast.csv')\n",
    "buffalo_forecast.to_csv(buffalo_forecast_path, index=False)\n",
    "\n",
    "# Save models and forecasts for each neighborhood\n",
    "for neighborhood, model in models.items():\n",
    "    forecast_path = os.path.join(forecast_directory, f'forecast_{neighborhood}.csv')\n",
    "    forecasts[neighborhood].to_csv(forecast_path, index=False)\n",
    "print(\"Models and forecasts saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
